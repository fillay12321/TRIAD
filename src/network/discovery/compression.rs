//! Data compression module using zstd and protobuf
//! Reduces network traffic by 3-5x

use std::sync::Arc;
use tokio::sync::Mutex;
use log::{debug, info, warn};
use serde::{Serialize, Deserialize};
use std::time::Instant;

use crate::network::error::NetworkError;

/// Compression algorithm types
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CompressionAlgorithm {
    /// No compression
    None,
    /// zstd compression (fast, good ratio)
    Zstd,
    /// LZ4 compression (very fast, moderate ratio)
    Lz4,
    /// Brotli compression (slower, better ratio)
    Brotli,
}

/// Compression level (0-22 for zstd, 0-11 for Brotli)
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct CompressionLevel(u8);

impl CompressionLevel {
    /// Creates new compression level
    pub fn new(level: u8) -> Result<Self, NetworkError> {
        match level {
            0..=22 => Ok(Self(level)),
            _ => Err(NetworkError::Internal("Invalid compression level".to_string())),
        }
    }
    
    /// Gets the level value
    pub fn value(&self) -> u8 {
        self.0
    }
    
    /// Default compression level (good balance)
    pub fn default() -> Self {
        Self(3)
    }
    
    /// Maximum compression level
    pub fn max() -> Self {
        Self(22)
    }
    
    /// Minimum compression level (fastest)
    pub fn min() -> Self {
        Self(0)
    }
}

/// Compression configuration
#[derive(Debug, Clone)]
pub struct CompressionConfig {
    /// Default compression algorithm
    pub algorithm: CompressionAlgorithm,
    /// Default compression level
    pub level: CompressionLevel,
    /// Minimum size to compress (bytes)
    pub min_size: usize,
    /// Maximum compression time (ms)
    pub max_time_ms: u64,
    /// Enable adaptive compression
    pub adaptive: bool,
    /// Compression statistics
    pub track_stats: bool,
}

impl Default for CompressionConfig {
    fn default() -> Self {
        Self {
            algorithm: CompressionAlgorithm::Zstd,
            level: CompressionLevel::default(),
            min_size: 1024, // 1KB minimum
            max_time_ms: 100, // 100ms max
            adaptive: true,
            track_stats: true,
        }
    }
}

/// Compressed data wrapper
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CompressedData {
    /// Compression algorithm used
    pub algorithm: u8,
    /// Compression level used
    pub level: u8,
    /// Original size
    pub original_size: usize,
    /// Compressed data
    pub data: Vec<u8>,
    /// Checksum for integrity
    pub checksum: u32,
}

/// Compression service
#[derive(Debug)]
pub struct CompressionService {
    /// Configuration
    config: CompressionConfig,
    /// Compression statistics
    stats: Arc<Mutex<CompressionStats>>,
    /// Running flag
    running: bool,
}

/// Compression statistics
#[derive(Debug, Default, Clone)]
struct CompressionStats {
    /// Total bytes compressed
    total_bytes_compressed: u64,
    /// Total bytes decompressed
    total_bytes_decompressed: u64,
    /// Total compression time (ms)
    total_compression_time_ms: u64,
    /// Total decompression time (ms)
    total_decompression_time_ms: u64,
    /// Compression operations count
    compression_count: u64,
    /// Decompression operations count
    decompression_count: u64,
    /// Average compression ratio
    avg_compression_ratio: f32,
}

impl CompressionService {
    /// Creates new compression service
    pub fn new(config: CompressionConfig) -> Self {
        Self {
            config,
            stats: Arc::new(Mutex::new(CompressionStats::default())),
            running: false,
        }
    }
    
    /// Starts compression service
    pub async fn start(&mut self) -> Result<(), NetworkError> {
        if self.running {
            return Err(NetworkError::ServiceAlreadyStarted);
        }
        
        info!("ðŸš€ Starting compression service with {:?} level {}", 
            self.config.algorithm, self.config.level.value());
        
        self.running = true;
        Ok(())
    }
    
    /// Stops compression service
    pub async fn stop(&mut self) -> Result<(), NetworkError> {
        if !self.running {
            return Ok(());
        }
        
        info!("ðŸ›‘ Stopping compression service...");
        
        // Print final statistics
        if self.config.track_stats {
            let stats = self.stats.lock().await;
            info!("ðŸ“Š Final compression stats: {:.2}x ratio, {} operations", 
                stats.avg_compression_ratio, stats.compression_count);
        }
        
        self.running = false;
        Ok(())
    }
    
    /// Compresses data using configured algorithm
    pub async fn compress(&self, data: &[u8]) -> Result<CompressedData, NetworkError> {
        if !self.running {
            return Err(NetworkError::ServiceAlreadyStarted);
        }
        
        // Check minimum size
        if data.len() < self.config.min_size {
            return Ok(CompressedData {
                algorithm: 0, // None
                level: 0,
                original_size: data.len(),
                data: data.to_vec(),
                checksum: self.calculate_checksum(data),
            });
        }
        
        let start_time = Instant::now();
        
        let (algorithm, level, compressed_data) = match self.config.algorithm {
            CompressionAlgorithm::None => {
                (0, 0, data.to_vec())
            }
            CompressionAlgorithm::Zstd => {
                let level = self.config.level.value();
                let compressed = zstd::encode_all(data, level as i32)
                    .map_err(|e| NetworkError::Internal(format!("zstd compression failed: {}", e)))?;
                (1, level, compressed)
            }
            CompressionAlgorithm::Lz4 => {
                let level = self.config.level.value();
                let compressed = lz4_flex::compress_prepend_size(data);
                (2, level, compressed)
            }
            CompressionAlgorithm::Brotli => {
                let level = self.config.level.value();
                let mut compressed = Vec::new();
                let compressed_clone = compressed.clone();
                let mut writer = brotli::CompressorWriter::new(&mut compressed, 4096, 22, level as u32);
                std::io::copy(&mut std::io::Cursor::new(data), &mut writer)
                    .map_err(|e| NetworkError::Internal(format!("Brotli compression failed: {}", e)))?;
                (3, level, compressed_clone)
            }
        };
        
        let compression_time = start_time.elapsed().as_millis() as u64;
        
        // Check time limit
        if compression_time > self.config.max_time_ms {
            warn!("âš ï¸ Compression took {}ms, exceeding limit of {}ms", 
                compression_time, self.config.max_time_ms);
            
            // Fall back to uncompressed
            return Ok(CompressedData {
                algorithm: 0,
                level: 0,
                original_size: data.len(),
                data: data.to_vec(),
                checksum: self.calculate_checksum(data),
            });
        }
        
        // Calculate compression ratio
        let ratio = if compressed_data.len() > 0 {
            data.len() as f32 / compressed_data.len() as f32
        } else {
            1.0
        };
        
        // Update statistics
        if self.config.track_stats {
            let mut stats = self.stats.lock().await;
            stats.total_bytes_compressed += data.len() as u64;
            stats.compression_count += 1;
            stats.total_compression_time_ms += compression_time;
            
            // Update average ratio
            let total_ratio = stats.avg_compression_ratio * (stats.compression_count - 1) as f32 + ratio;
            stats.avg_compression_ratio = total_ratio / stats.compression_count as f32;
        }
        
        let result = CompressedData {
            algorithm,
            level,
            original_size: data.len(),
            data: compressed_data,
            checksum: self.calculate_checksum(data),
        };
        
        debug!("ðŸ“¦ Compressed {} bytes to {} bytes ({:.2}x ratio) in {}ms", 
            data.len(), result.data.len(), ratio, compression_time);
        
        Ok(result)
    }
    
    /// Decompresses data
    pub async fn decompress(&self, compressed: &CompressedData) -> Result<Vec<u8>, NetworkError> {
        if !self.running {
            return Err(NetworkError::ServiceAlreadyStarted);
        }
        
        let start_time = Instant::now();
        
        let decompressed = match compressed.algorithm {
            0 => {
                // No compression
                compressed.data.clone()
            }
            1 => {
                // zstd
                zstd::decode_all(&compressed.data[..])
                    .map_err(|e| NetworkError::Internal(format!("zstd decompression failed: {}", e)))?
            }
            2 => {
                // LZ4
                lz4_flex::decompress_size_prepended(&compressed.data)
                    .map_err(|e| NetworkError::Internal(format!("LZ4 decompression failed: {}", e)))?
            }
            3 => {
                // Brotli
                let mut decompressed = Vec::new();
                let mut reader = brotli::Decompressor::new(&compressed.data[..], 4096);
                std::io::copy(&mut reader, &mut decompressed)
                    .map_err(|e| NetworkError::Internal(format!("Brotli decompression failed: {}", e)))?;
                decompressed
            }
            _ => {
                return Err(NetworkError::Internal("Unknown compression algorithm".to_string()));
            }
        };
        
        let decompression_time = start_time.elapsed().as_millis() as u64;
        
        // Verify size
        if decompressed.len() != compressed.original_size {
            return Err(NetworkError::Internal(format!(
                "Decompressed size {} doesn't match original size {}", 
                decompressed.len(), compressed.original_size
            )));
        }
        
        // Verify checksum
        let calculated_checksum = self.calculate_checksum(&decompressed);
        if calculated_checksum != compressed.checksum {
            return Err(NetworkError::Internal("Checksum verification failed".to_string()));
        }
        
        // Update statistics
        if self.config.track_stats {
            let mut stats = self.stats.lock().await;
            stats.total_bytes_decompressed += decompressed.len() as u64;
            stats.decompression_count += 1;
            stats.total_decompression_time_ms += decompression_time;
        }
        
        debug!("ðŸ“¦ Decompressed {} bytes from {} bytes in {}ms", 
            decompressed.len(), compressed.data.len(), decompression_time);
        
        Ok(decompressed)
    }
    
    /// Compresses data with specific algorithm and level
    pub async fn compress_with(&self, data: &[u8], algorithm: CompressionAlgorithm, level: CompressionLevel) -> Result<CompressedData, NetworkError> {
        // Temporarily change config
        let original_algorithm = self.config.algorithm;
        let original_level = self.config.level;
        
        let mut temp_config = self.config.clone();
        temp_config.algorithm = algorithm;
        temp_config.level = level;
        
        let mut temp_service = CompressionService::new(temp_config);
        temp_service.start().await?;
        
        let result = temp_service.compress(data).await;
        
        temp_service.stop().await?;
        
        result
    }
    
    /// Gets compression statistics
    pub async fn get_stats(&self) -> CompressionStats {
        self.stats.lock().await.clone()
    }
    
    /// Calculates simple checksum for data integrity
    fn calculate_checksum(&self, data: &[u8]) -> u32 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        data.hash(&mut hasher);
        hasher.finish() as u32
    }
    
    /// Estimates compression ratio for given data
    pub async fn estimate_compression_ratio(&self, data: &[u8]) -> f32 {
        if data.len() < self.config.min_size {
            return 1.0;
        }
        
        // Sample compression for estimation
        let sample_size = std::cmp::min(data.len(), 1024);
        let sample = &data[..sample_size];
        
        match self.compress(sample).await {
            Ok(compressed) => {
                let ratio = sample.len() as f32 / compressed.data.len() as f32;
                ratio
            }
            Err(_) => {
                // Fallback to historical average
                let stats = self.stats.lock().await;
                stats.avg_compression_ratio
            }
        }
    }
    
    /// Adaptive compression based on data characteristics
    pub async fn adaptive_compress(&self, data: &[u8]) -> Result<CompressedData, NetworkError> {
        if !self.config.adaptive {
            return self.compress(data).await;
        }
        
        // Analyze data characteristics
        let entropy = self.calculate_entropy(data);
        let repeat_patterns = self.detect_repeat_patterns(data);
        
        // Choose algorithm based on characteristics
        let (algorithm, level) = if entropy < 0.3 {
            // Low entropy data - good for compression
            (CompressionAlgorithm::Zstd, CompressionLevel::max())
        } else if repeat_patterns > 0.7 {
            // High repeat patterns - good for compression
            (CompressionAlgorithm::Brotli, CompressionLevel::new(9)?)
        } else if data.len() < 4096 {
            // Small data - use fast compression
            (CompressionAlgorithm::Lz4, CompressionLevel::min())
        } else {
            // Default case
            (self.config.algorithm, self.config.level)
        };
        
        self.compress_with(data, algorithm, level).await
    }
    
    /// Calculates data entropy (0.0 = no entropy, 1.0 = maximum entropy)
    fn calculate_entropy(&self, data: &[u8]) -> f32 {
        if data.is_empty() {
            return 0.0;
        }
        
        let mut byte_counts = [0u32; 256];
        for &byte in data {
            byte_counts[byte as usize] += 1;
        }
        
        let len = data.len() as f32;
        let mut entropy = 0.0;
        
        for &count in &byte_counts {
            if count > 0 {
                let probability = count as f32 / len;
                entropy -= probability * probability.log2();
            }
        }
        
        entropy / 8.0 // Normalize to 0-1 range
    }
    
    /// Detects repeat patterns in data (0.0 = no patterns, 1.0 = many patterns)
    fn detect_repeat_patterns(&self, data: &[u8]) -> f32 {
        if data.len() < 16 {
            return 0.0;
        }
        
        let mut patterns = 0;
        let mut total_checks = 0;
        
        // Check for repeated 4-byte patterns
        for i in 0..data.len().saturating_sub(4) {
            let pattern = &data[i..i+4];
            
            for j in i+4..data.len().saturating_sub(4) {
                total_checks += 1;
                if &data[j..j+4] == pattern {
                    patterns += 1;
                }
            }
        }
        
        if total_checks == 0 {
            return 0.0;
        }
        
        patterns as f32 / total_checks as f32
    }
}

/// Protobuf serialization wrapper
#[derive(Debug)]
pub struct ProtobufService {
    /// Enable compression
    enable_compression: bool,
    /// Compression service
    compression: CompressionService,
}

impl ProtobufService {
    /// Creates new protobuf service
    pub fn new(enable_compression: bool) -> Self {
        let config = CompressionConfig {
            algorithm: CompressionAlgorithm::Zstd,
            level: CompressionLevel::new(3).unwrap(),
            min_size: 512, // Lower threshold for protobuf
            max_time_ms: 50, // Faster compression for protobuf
            adaptive: false,
            track_stats: true,
        };
        
        Self {
            enable_compression,
            compression: CompressionService::new(config),
        }
    }
    
    /// Serializes data to protobuf with optional compression
    pub async fn serialize<T: Serialize>(&self, data: &T) -> Result<Vec<u8>, NetworkError> {
        // TODO: Implement actual protobuf serialization
        // For now, use bincode as placeholder
        
        let serialized = bincode::serialize(data)
            .map_err(|e| NetworkError::Internal(format!("Serialization failed: {}", e)))?;
        
        if self.enable_compression {
            self.compression.compress(&serialized).await.map(|c| c.data)
        } else {
            Ok(serialized)
        }
    }
    
    /// Deserializes data from protobuf with optional decompression
    pub async fn deserialize<T: for<'de> Deserialize<'de>>(&self, data: &[u8]) -> Result<T, NetworkError> {
        // TODO: Implement actual protobuf deserialization
        // For now, use bincode as placeholder
        
        let decompressed = if self.enable_compression {
            // Try to detect if data is compressed
            if data.len() > 0 && data[0] != 0 {
                // Assume compressed
                let compressed = CompressedData {
                    algorithm: 1, // zstd
                    level: 3,
                    original_size: 0, // Will be set by decompress
                    data: data.to_vec(),
                    checksum: 0, // Will be verified by decompress
                };
                self.compression.decompress(&compressed).await?
            } else {
                data.to_vec()
            }
        } else {
            data.to_vec()
        };
        
        bincode::deserialize(&decompressed)
            .map_err(|e| NetworkError::Internal(format!("Deserialization failed: {}", e)))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_compression_service() {
        let config = CompressionConfig::default();
        let mut service = CompressionService::new(config);
        service.start().await.unwrap();
        
        let test_data = b"Hello, world! This is a test message for compression.".repeat(100);
        
        // Test compression
        let compressed = service.compress(&test_data).await.unwrap();
        assert!(compressed.data.len() < test_data.len());
        
        // Test decompression
        let decompressed = service.decompress(&compressed).await.unwrap();
        assert_eq!(decompressed, test_data);
        
        service.stop().await.unwrap();
    }
    
    #[tokio::test]
    async fn test_adaptive_compression() {
        let config = CompressionConfig {
            adaptive: true,
            ..Default::default()
        };
        let mut service = CompressionService::new(config);
        service.start().await.unwrap();
        
        // Test with different data types
        let random_data: Vec<u8> = (0..1000).map(|_| rand::random::<u8>()).collect();
        let repeated_data = b"ABC".repeat(1000);
        
        let random_compressed = service.adaptive_compress(&random_data).await.unwrap();
        let repeated_compressed = service.adaptive_compress(&repeated_data).await.unwrap();
        
        // Repeated data should compress better
        let random_ratio = random_data.len() as f32 / random_compressed.data.len() as f32;
        let repeated_ratio = repeated_data.len() as f32 / repeated_compressed.data.len() as f32;
        
        assert!(repeated_ratio > random_ratio);
        
        service.stop().await.unwrap();
    }
}
